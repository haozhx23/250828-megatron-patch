apiVersion: trainer.kubeflow.org/v1alpha1
kind: ClusterTrainingRuntime
metadata:
  name: torch-distributed-megatron
spec:
  mlPolicy:
    numNodes: 1
    torch:
      numProcPerNode: auto
  template:
    spec:
      replicatedJobs:
      - name: node
        replicas: 1
        template:
          metadata:
            labels:
              trainer.kubeflow.org/trainjob-ancestor-step: trainer
          spec:
            template:
              spec:
                nodeSelector:
                  node.kubernetes.io/instance-type: "p6-b200.48xlarge"
                volumes:
                - name: fsx-storage
                  persistentVolumeClaim:
                    claimName: fsx-claim
                - name: shared-memory
                  emptyDir:
                    medium: Memory
                    sizeLimit: 8Gi
                containers:
                - name: node
                  image: public.ecr.aws/t5u4s6i0/nvcr-2503-efa-mg0140:latest
                  workingDir: /workspace/megatron-lm
                  volumeMounts:
                  # Megatron-LM 代码目录
                  - name: fsx-storage
                    mountPath: /workspace/megatron-lm
                    subPath: mgtlm_workspace/Megatron-LM
                  # Checkpoints 目录
                  - name: fsx-storage
                    mountPath: /workspace/checkpoints
                    subPath: mgtlm_workspace/checkpoints
                  # TensorBoard 日志目录
                  - name: fsx-storage
                    mountPath: /workspace/tensorboard_logs
                    subPath: mgtlm_workspace/tensorboard_logs
                  # 数据集目录
                  - name: fsx-storage
                    mountPath: /workspace/built_datasets
                    subPath: mgtlm_workspace/built_datasets
                  # 共享内存
                  - name: shared-memory
                    mountPath: /dev/shm
                  resources:
                    requests:
                      nvidia.com/gpu: 8
                      vpc.amazonaws.com/efa: 8
                    limits:
                      nvidia.com/gpu: 8
                      vpc.amazonaws.com/efa: 8
                  env:
                  - name: NCCL_DEBUG
                    value: "INFO"
                  # - name: FI_EFA_USE_DEVICE_RDMA
                  #   value: "1"
                  # - name: FI_PROVIDER
                  #   value: "efa"
                  # - name: FI_LOG_LEVEL
                  #   value: "warn"
---
apiVersion: trainer.kubeflow.org/v1alpha1
kind: TrainJob
metadata:
  name: megatron-multinode
spec:
  trainer:
    numNodes: 2
    numProcPerNode: 8
    image: public.ecr.aws/t5u4s6i0/nvcr-2503-efa-mg0140:latest
    command:
      - "bash"
      - "train-llama-elastic.sh"
    env:
    - name: JOBSET_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.labels['jobset.sigs.k8s.io/jobset-name']
    # Megatron 环境变量
    - name: NCCL_DEBUG
      value: "INFO"
    - name: FI_EFA_USE_DEVICE_RDMA
      value: "1"
    - name: FI_PROVIDER
      value: "efa"
    - name: FI_LOG_LEVEL
      value: "warn"
    resourcesPerNode:
      requests:
        nvidia.com/gpu: 8
        vpc.amazonaws.com/efa: 8
      limits:
        nvidia.com/gpu: 8
        vpc.amazonaws.com/efa: 8
  runtimeRef:
    name: torch-distributed-megatron
    apiGroup: trainer.kubeflow.org
    kind: ClusterTrainingRuntime
